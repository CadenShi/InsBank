{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [128000, 15339, 1917], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../allmodels/Meta-Llama-3-8B-base-original\")\n",
    "tokenizer('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jload(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        f.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def jdump(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def log(out_path):\n",
    "    with open('../LLaMA-Factory/data/dataset_info.json', 'r') as f:\n",
    "        dataset_info = json.load(f)\n",
    "    name = out_path.split('/')[-1].split('.json')[0]\n",
    "    dataset_info[name] = {\n",
    "        \"file_name\": f\"{name}.json\",\n",
    "        \"formatting\": \"sharegpt\"\n",
    "    }\n",
    "    with open('../LLaMA-Factory/data/dataset_info.json', 'w') as f:\n",
    "        json.dump(dataset_info, f, indent=2)\n",
    "        \n",
    "\n",
    "def get_valid_token_len_and_conv_turn(conv, max_length=2048):\n",
    "    \"\"\"Get the effective train token length and conversation turns\n",
    "    params\n",
    "        conv: complete conversation\n",
    "        max_length: max_length set in trainer\n",
    "    \"\"\"\n",
    "    valid_token = 0\n",
    "    valid_train_token = 0\n",
    "    valid_turn = 0\n",
    "    for turn in conv:\n",
    "        cur_sent_token = len(tokenizer(turn['value'])['input_ids'])\n",
    "        if valid_train_token + cur_sent_token > max_length:\n",
    "            break\n",
    "        if turn['from'] == 'gpt':\n",
    "            valid_turn += 1\n",
    "            valid_train_token += cur_sent_token\n",
    "    return valid_train_token, valid_turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_token_budget_adjust(conversations, max_length=2048, token_budget=3000000, turn_budget=7000):\n",
    "    \"\"\"Adjust the train data to fit token budget\n",
    "    params\n",
    "        conversations: raw conversation data\n",
    "        max_length: max_length set in trainer\n",
    "        token_budget: the training token budget\n",
    "    \"\"\"\n",
    "    valid_conversations = []\n",
    "    total_valid_turns = 0\n",
    "    valid_train_token = 0\n",
    "    for conv in conversations:\n",
    "        total_token_len = 0\n",
    "        tmp_valid_turns = []\n",
    "        for turn in conv['conversations']:\n",
    "            cur_sent_token = len(tokenizer(turn['value'])['input_ids'])\n",
    "            if total_token_len + cur_sent_token > max_length:\n",
    "                break\n",
    "            total_token_len += cur_sent_token\n",
    "            if turn['from'] == 'gpt':\n",
    "                valid_train_token += cur_sent_token\n",
    "                if valid_train_token + cur_sent_token > token_budget:\n",
    "                    break\n",
    "                total_valid_turns += 1\n",
    "            tmp_valid_turns.append(turn)\n",
    "\n",
    "        while len(tmp_valid_turns) > 2 and tmp_valid_turns[-1]['from'] != 'gpt':\n",
    "            tmp_valid_turns = tmp_valid_turns[:-1]\n",
    "        valid_conversations.append({\"conversations\": tmp_valid_turns})\n",
    "\n",
    "        if valid_train_token >= token_budget or total_valid_turns>=turn_budget:\n",
    "            break\n",
    "    return valid_conversations, total_valid_turns, valid_train_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:07,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "445347\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:09<00:10,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "1733819\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:15<00:05,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999\n",
      "1755852\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:26<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7003\n",
      "2570873\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_self_instruct\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_alpaca\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_dolly\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_sharegpt\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    # QC. Preliminary: token_budget=2041300, turn_budget=7000\n",
    "    valid_conversations, valid_turn, valid_train_token = train_token_budget_adjust(data)\n",
    "    jdump(valid_conversations, f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "    log(f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "\n",
    "    print(valid_turn)\n",
    "    print(valid_train_token)\n",
    "    print(\"--\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:06,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445347\n",
      "6000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:08<00:08,  4.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1733819\n",
      "6000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:14<00:05,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755852\n",
      "5999\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:25<00:08,  8.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m total_valid_turn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 26\u001b[0m     valid_train_token, valid_turn \u001b[38;5;241m=\u001b[39m \u001b[43mget_valid_token_len_and_conv_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconversations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     total_train_token \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_train_token\n\u001b[1;32m     28\u001b[0m     total_valid_turn \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m valid_turn\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mget_valid_token_len_and_conv_turn\u001b[0;34m(conv, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m valid_turn \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m turn \u001b[38;5;129;01min\u001b[39;00m conv:\n\u001b[0;32m---> 36\u001b[0m     cur_sent_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mturn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_train_token \u001b[38;5;241m+\u001b[39m cur_sent_token \u001b[38;5;241m>\u001b[39m max_length:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2859\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2860\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2970\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2949\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2950\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2967\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2968\u001b[0m     )\n\u001b[1;32m   2969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3046\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3037\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3038\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3039\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3043\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3044\u001b[0m )\n\u001b[0;32m-> 3046\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3047\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3048\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3049\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:600\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    578\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    598\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    599\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 600\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py:526\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 526\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    538\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    540\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    550\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_self_instruct\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_alpaca\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_dolly\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_sharegpt\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    total_train_token = 0\n",
    "    total_valid_turn = 0\n",
    "    for conv in data:\n",
    "        valid_train_token, valid_turn = get_valid_token_len_and_conv_turn(conv['conversations'])\n",
    "        total_train_token += valid_train_token\n",
    "        total_valid_turn += valid_turn\n",
    "    \n",
    "    stats['name'].append(name)\n",
    "    stats['valid_train_token'].append(total_train_token)\n",
    "    stats['valid_conversations_turns'].append(total_valid_turn)\n",
    "\n",
    "    print(total_train_token)\n",
    "    print(total_valid_turn)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "# df = pd.DataFrame(stats)\n",
    "# df.to_csv('../analysis/train_data_stats.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:05<00:10,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313\n",
      "900236\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:09<00:04,  4.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301\n",
      "900089\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:13<00:00,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291\n",
      "900101\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    \"first_2k_1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    \"second_2k_1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    \"third_2k_1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\"\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    valid_conversations, valid_turn, valid_train_token = train_token_budget_adjust(data, 2048, 900000, 2700)\n",
    "    jdump(valid_conversations, f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "    log(f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "\n",
    "    print(valid_turn)\n",
    "    print(valid_train_token)\n",
    "    print(\"--\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:02<00:07,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445347\n",
      "6000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:08<00:09,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1733819\n",
      "6000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:14<00:05,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1755852\n",
      "5999\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:28<00:00,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670416\n",
      "10201\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    # \"budget_adjusted_cleaned_5sets_knn_multiply_gamma_1\",\n",
    "    # \"budget_adjusted_cleaned_5sets_knn_multiply_gamma_2\",\n",
    "    # \"budget_adjusted_cleaned_5sets_kcenter_multiply_gamma_1\",\n",
    "    # \"budget_adjusted_cleaned_5sets_kcenter_multiply_gamma_2\",\n",
    "    # \"budget_adjusted_cleaned_no_complexity_deita_6k\",\n",
    "    # \"budget_adjusted_1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_self_instruct\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_alpaca\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_dolly\",\n",
    "    \"1223_cleaned_5sets_euclidean_nonlinear_v4_0.95_alpha_0.3_lambda_0.9_gamma_2.0_6k_sharegpt\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    total_train_token = 0\n",
    "    total_valid_turn = 0\n",
    "    for conv in data:\n",
    "        valid_train_token, valid_turn = get_valid_token_len_and_conv_turn(conv['conversations'])\n",
    "        total_train_token += valid_train_token\n",
    "        total_valid_turn += valid_turn\n",
    "    \n",
    "    stats['name'].append(name)\n",
    "    stats['valid_train_token'].append(total_train_token)\n",
    "    stats['valid_conversations_turns'].append(total_valid_turn)\n",
    "\n",
    "    print(total_train_token)\n",
    "    print(total_valid_turn)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "df.to_csv('../analysis/train_data_stats.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:06<00:52,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2284267\n",
      "7005\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:13<00:49,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519609\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:21<00:44,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2291435\n",
      "6125\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:31<00:41,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2116656\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:39<00:32,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2568780\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:47<00:24,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2479524\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:56<00:16,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2707970\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [01:04<00:08,  8.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2587597\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:13<00:00,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2786629\n",
      "7000\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    # \"budget_adjusted_v2_cleaned_5sets_knn_multiply_gamma_1\",\n",
    "    # \"budget_adjusted_v2_cleaned_5sets_knn_multiply_gamma_2\",\n",
    "    # \"budget_adjusted_v2_cleaned_5sets_kcenter_multiply_gamma_1\",\n",
    "    # \"budget_adjusted_v2_cleaned_5sets_kcenter_multiply_gamma_2\",\n",
    "    # \"budget_adjusted_v2_cleaned_no_complexity_deita_6k\",\n",
    "    # \"budget_adjusted_v2_1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_v2_1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_v2_1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"budget_adjusted_v2_1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    total_train_token = 0\n",
    "    total_valid_turn = 0\n",
    "    for conv in data:\n",
    "        valid_train_token, valid_turn = get_valid_token_len_and_conv_turn(conv['conversations'])\n",
    "        total_train_token += valid_train_token\n",
    "        total_valid_turn += valid_turn\n",
    "    \n",
    "    stats['name'].append(name)\n",
    "    stats['valid_train_token'].append(total_train_token)\n",
    "    stats['valid_conversations_turns'].append(total_valid_turn)\n",
    "\n",
    "    print(total_train_token)\n",
    "    print(total_valid_turn)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "df.to_csv('../analysis/train_data_stats.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:12<01:41, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4689489\n",
      "14489\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:24<01:26, 12.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3975417\n",
      "11161\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:32<01:00, 10.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343499\n",
      "6162\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:43<00:52, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2412402\n",
      "7720\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:52<00:40, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3112491\n",
      "8561\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [01:03<00:30, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3147849\n",
      "8751\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [01:12<00:19,  9.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3106774\n",
      "7988\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [01:57<00:21, 21.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3114606\n",
      "8299\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [02:08<00:00, 14.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3094305\n",
      "7751\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    \"cleaned_5sets_knn_multiply_gamma_1\",\n",
    "    \"cleaned_5sets_knn_multiply_gamma_2\",\n",
    "    \"cleaned_5sets_kcenter_multiply_gamma_1\",\n",
    "    \"cleaned_5sets_kcenter_multiply_gamma_2\",\n",
    "    \"cleaned_no_complexity_deita_6k\",\n",
    "    \"1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    \"1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    \"1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    \"1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    total_train_token = 0\n",
    "    total_valid_turn = 0\n",
    "    for conv in data:\n",
    "        valid_train_token, valid_turn = get_valid_token_len_and_conv_turn(conv['conversations'])\n",
    "        total_train_token += valid_train_token\n",
    "        total_valid_turn += valid_turn\n",
    "    \n",
    "    stats['name'].append(name)\n",
    "    stats['valid_train_token'].append(total_train_token)\n",
    "    stats['valid_conversations_turns'].append(total_valid_turn)\n",
    "\n",
    "    print(total_train_token)\n",
    "    print(total_valid_turn)\n",
    "    print(\"--\"*20)\n",
    "\n",
    "df = pd.DataFrame(stats)\n",
    "df.to_csv('../analysis/train_data_stats.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:08<00:24,  8.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6109\n",
      "2286630\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:18<00:18,  9.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "1793077\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:26<00:08,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "2296054\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7001\n",
      "2553184\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "names = (\n",
    "    # \"cleaned_5sets_knn_multiply_gamma_1\",\n",
    "    # \"cleaned_5sets_knn_multiply_gamma_2\",\n",
    "    # \"cleaned_5sets_kcenter_multiply_gamma_1\",\n",
    "    # \"cleaned_5sets_kcenter_multiply_gamma_2\",\n",
    "    # \"cleaned_no_complexity_deita_6k\",\n",
    "    # \"1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    # \"1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k_wizardlm\",\n",
    "    # \"1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k_wizardlm\",\n",
    "    \"cleaned_5sets_kcenter_addition_gamma_1\",\n",
    "    \"cleaned_5sets_kcenter_addition_gamma_2\",\n",
    "    \"cleaned_5sets_knn_addition_gamma_1\",\n",
    "    \"cleaned_5sets_knn_addition_gamma_2\",\n",
    ")\n",
    "\n",
    "stats = {\n",
    "    'name': [],\n",
    "    'valid_train_token': [],\n",
    "    'valid_conversations_turns': []\n",
    "}\n",
    "\n",
    "for name in tqdm(names):\n",
    "    path=f'../LLaMA-Factory/data/{name}.json'\n",
    "    data = jload(path)\n",
    "\n",
    "    valid_conversations, valid_turn, valid_train_token = train_token_budget_adjust(data)\n",
    "    jdump(valid_conversations, f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "    log(f\"../LLaMA-Factory/data/budget_adjusted_v2_{name}.json\")\n",
    "\n",
    "    print(valid_turn)\n",
    "    print(valid_train_token)\n",
    "    print(\"--\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_token_budget_adjust_pth(path, max_length=2048, token_budget=3000000):\n",
    "    \"\"\"Adjust the train data to fit token budget\n",
    "    params\n",
    "        conversations: raw conversation data\n",
    "        max_length: max_length set in trainer\n",
    "        token_budget: the training token budget\n",
    "    \"\"\"\n",
    "    data = torch.load(path)['data']\n",
    "\n",
    "    valid_conv_quality = []\n",
    "    total_valid_turns = 0\n",
    "    valid_train_token = 0\n",
    "    for i in range(len(data)):\n",
    "        conv = data[i]\n",
    "        total_token_len = 0\n",
    "        tmp_valid_turns = []\n",
    "        for turn in conv['conversations']:\n",
    "            cur_sent_token = len(tokenizer(turn['value'])['input_ids'])\n",
    "            if total_token_len + cur_sent_token > max_length:\n",
    "                break\n",
    "            total_token_len += cur_sent_token\n",
    "            if turn['from'] not in ['human', 'user', 'system']:\n",
    "                valid_train_token += cur_sent_token\n",
    "                if valid_train_token + cur_sent_token > token_budget:\n",
    "                    break\n",
    "                total_valid_turns += 1\n",
    "            tmp_valid_turns.append(turn)\n",
    "\n",
    "        while len(tmp_valid_turns) > 2 and tmp_valid_turns[-1]['from'] in ['human', 'user', 'system']:\n",
    "            tmp_valid_turns = tmp_valid_turns[:-1]\n",
    "        valid_conv_quality.append(data[i]['quality'])\n",
    "\n",
    "        if valid_train_token >= token_budget or total_valid_turns>=7000:\n",
    "            break\n",
    "    avg_quality = sum(valid_conv_quality) / len(valid_conv_quality)\n",
    "    return avg_quality, total_valid_turns, valid_train_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:07<00:52,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.860175119071972\n",
      "7000\n",
      "2296054\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:16<00:51,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.160868338225023\n",
      "7000\n",
      "2553184\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:24<00:40,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.847259938568849\n",
      "7000\n",
      "2284267\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:33<00:33,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.136837000070238\n",
      "7000\n",
      "2519609\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:44<00:28,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1235864869229735\n",
      "7000\n",
      "2479524\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:53<00:18,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.307052951961695\n",
      "7000\n",
      "2707970\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [01:03<00:09,  9.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2235377209058065\n",
      "7000\n",
      "2587597\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:14<00:00,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.34572109318431\n",
      "7000\n",
      "2786629\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paths = (\n",
    "    \"../baselines/cleaned_5sets_knn_addition_gamma_1.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_addition_gamma_2.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_multiply_gamma_1.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_multiply_gamma_2.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k/WizardLM_alpaca.pth\",\n",
    ")\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    avg_quality, total_valid_turns, valid_train_token = train_token_budget_adjust_pth(path)\n",
    "    print(avg_quality)\n",
    "    print(total_valid_turns)\n",
    "    print(valid_train_token)\n",
    "    print(\"--\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_token_budget_adjust_pth(path, max_length=2048, token_budget=3000000):\n",
    "    \"\"\"Adjust the train data to fit token budget\n",
    "    params\n",
    "        conversations: raw conversation data\n",
    "        max_length: max_length set in trainer\n",
    "        token_budget: the training token budget\n",
    "    \"\"\"\n",
    "    data = torch.load(path)['data']\n",
    "\n",
    "    valid_conv_quality = []\n",
    "    total_valid_turns = 0\n",
    "    valid_train_token = 0\n",
    "    for i in range(len(data)):\n",
    "        conv = data[i]\n",
    "        total_token_len = 0\n",
    "        tmp_valid_turns = []\n",
    "        for turn in conv['conversations']:\n",
    "            cur_sent_token = len(tokenizer(turn['value'])['input_ids'])\n",
    "            if total_token_len + cur_sent_token > max_length:\n",
    "                break\n",
    "            total_token_len += cur_sent_token\n",
    "            if turn['from'] not in ['human', 'user', 'system']:\n",
    "                valid_train_token += cur_sent_token\n",
    "                if valid_train_token + cur_sent_token > token_budget:\n",
    "                    break\n",
    "                total_valid_turns += 1\n",
    "            tmp_valid_turns.append(turn)\n",
    "\n",
    "        while len(tmp_valid_turns) > 2 and tmp_valid_turns[-1]['from'] in ['human', 'user', 'system']:\n",
    "            tmp_valid_turns = tmp_valid_turns[:-1]\n",
    "        valid_conv_quality.append(data[i]['quality'])\n",
    "\n",
    "        if valid_train_token >= token_budget:\n",
    "            break\n",
    "    avg_quality = sum(valid_conv_quality) / len(valid_conv_quality)\n",
    "    return avg_quality, total_valid_turns, valid_train_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:09<01:06,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.854001088280239\n",
      "9211\n",
      "3000009\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:18<00:56,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.148246010369022\n",
      "8328\n",
      "3000258\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:28<00:46,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.839619930337753\n",
      "9236\n",
      "3000344\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:37<00:37,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.121643718742637\n",
      "8406\n",
      "3000493\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [00:48<00:29,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.131396633689667\n",
      "8375\n",
      "3000475\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [00:58<00:19,  9.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3044659973302695\n",
      "7743\n",
      "3000153\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [01:09<00:10, 10.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.225794443713172\n",
      "8023\n",
      "3000046\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:19<00:00,  9.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.341632059033741\n",
      "7533\n",
      "3000563\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paths = (\n",
    "    \"../baselines/cleaned_5sets_knn_addition_gamma_1.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_addition_gamma_2.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_multiply_gamma_1.pth\",\n",
    "    \"../baselines/cleaned_5sets_knn_multiply_gamma_2.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_1.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1205_cleaned_euclidean_multiply_alpha_0.3_lambda_0.9_gamma_2.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_1.0_6k/WizardLM_alpaca.pth\",\n",
    "    \"../pool_evolve/ap_outputs/1209_cleaned_euclidean_addition_alpha_0.3_lambda_0.9_gamma_2.0_6k/WizardLM_alpaca.pth\",\n",
    ")\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    avg_quality, total_valid_turns, valid_train_token = train_token_budget_adjust_pth(path)\n",
    "    print(avg_quality)\n",
    "    print(total_valid_turns)\n",
    "    print(valid_train_token)\n",
    "    print(\"--\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
